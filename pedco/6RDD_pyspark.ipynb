{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c7a638-3d88-428c-a424-1ff74dfa46d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'probandospark6rdd'),\n",
       " ('spark.driver.host', '192.168.1.42'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/usuarioFAI/ejemplosPython/spark-warehouse'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.driver.port', '34087'),\n",
       " ('spark.app.startTime', '1620874913349'),\n",
       " ('spark.executor.memory', '1g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.instances', '1'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'spark://bigdata-srv.fi.uncoma.edu.ar:7077'),\n",
       " ('spark.app.id', 'app-20210513000155-0006'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.memoryOverhead', '1g')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importar hdfs\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "\n",
    "#abro una sesion con un nombre\n",
    "#hay que tener cuidado que los workers de spark tengan la cantidad de Mb para ejecutar\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".config(\"spark.submit.deployMode\", \"client\")\\\n",
    ".config(\"spark.executor.instances\", \"1\")\\\n",
    ".config(\"spark.executor.memory\", \"1g\")\\\n",
    ".config(\"spark.driver.memory\", \"1g\")\\\n",
    ".config(\"spark.executor.memoryOverhead\", \"1g\")\\\n",
    ".appName(\"probandospark6rdd\")\\\n",
    ".master(\"spark://bigdata-srv.fi.uncoma.edu.ar:7077\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext._conf.getAll()  # ver la configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f5900e-2299-4a2e-98ea-aaf70f302fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     \n",
    "def otraPruebaRDDdesdeHDFS():  \n",
    "    distFile = spark.sparkContext.textFile('hdfs://127.0.0.1:9000/cursoFAI/nba.csv')\n",
    "    counts=distFile.flatMap(lambda x: x.split(\" \")).map(lambda x: (x,1)).reduceByKey(add)\n",
    "    \n",
    "    #si existe borrarla\n",
    "    #conectarme\n",
    "    client = InsecureClient('http://localhost:9870', user='usuarioFAI')\n",
    "    #Borrar un archivo o carpeta - sino esta no hace nada\n",
    "    borrar = client.delete('/cursoFAI/fuente1Modificada.txt',recursive=True)\n",
    "        \n",
    "    counts.saveAsTextFile('hdfs://127.0.0.1:9000/cursoFAI/fuente1Modificada.txt')\n",
    "    #esto que comente lista el archivo y es muy grande!\n",
    "    #for (word, count) in output:\n",
    "     #   print(word,\",\",count)\n",
    "    \n",
    "otraPruebaRDDdesdeHDFS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33578e68-d06b-4904-ad95-f30be610e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cierro la sesión\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfc71a-23c5-413e-91b3-a866927b0667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
